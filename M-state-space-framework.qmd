# State space framework {#m-ssf .unnumbered}

## 1. Overview

### 1.1 Description of the general model

The `general linear gaussian` state-space model can be written in many different ways. The form considered in JD+ ($\ge 3.0$) is presented below:

$$ y_t = Z_t \alpha_t + \epsilon_t,\quad \epsilon_t \sim N\left(0, \sigma^2 H_t\right),\quad t \gt 0 $$

$$ \alpha_{t+1} = T_t \alpha_t + \mu_t, \quad \mu_t \sim N \left(0, \sigma^2 V_t \right),\quad t \ge 0 $$

$y_t$ is the observation at period t, $\alpha_t$ is the state vector. $\epsilon_t, \mu_t$ are assumed to be serially independent at all leads and lags and independent from each other.\
In the case of multi-variate models, $y_t$ is a vector of observations. However, in most cases, we will use the univariate approach by considering the observations one by one (univariate handling of multi-variate models).

The innovations of the state equation will be modelled as

$$ \mu_t = S_t \xi_t, \quad \xi_t \sim N\left( 0, \sigma^2 I\right) $$

In other words, $V_t=S_t S_t'$

The initial ($\equiv t=0$) conditions of the filter are defined as follows:

$$ \alpha_{0} = a_{0} + B\delta + \mu_{0}, \quad \delta \sim N\left(0, \kappa I \right),\: \mu_{0} \sim N\left(0, P_*\right)$$

where $\kappa$ is arbitrary large. $P_*$ is the variance of the stationary part of the initial state vector and $B$ models the diffuse part. We write $BB'=P_\infty$.

The definition used in JD+ is quasi-identical to that of Durbin and Koopman\[1\].

In summary, the model is completely defined by the following quantities (possible default values are indicated in brackets):

$$ \mathbf{Z_t}, \mathbf{H_t} [=0] $$

$$ \mathbf{T_t}, \mathbf{V_t} [=S_t S_t'], \mathbf{S_t} [=Cholesky(V)] $$

$$ \mathbf{a_{0}}[=0], \mathbf{P_*} [=0], \mathbf{B} [=0], \mathbf{P_\infty} [=BB'] $$

### 1.2 Other notations

The following notations are used in all the pages related to the ssf framework:

$$ a_{t+k|t}=E\left(\alpha_{t+k} | y_0 \cdots y_{t}\right)$$

$$ P_{t+k|t}=var\left(\alpha_{t+k} | y_0 \cdots y_{t}\right)$$

$$ a_{t+1}=a_{t+1|t}, \: P_{t+1}=P_{t+1|t} $$

Finally, it should be noted that all the indexes of arrays or matrices start at 0 (Java and C++ convention).

### 1.3 Functional forms

State space forms and the related algorithms focus on the state vectors and their (conditional) distribution, and on the relationships between those vectors at different time points. For instance, using obvious notations, we will consider:

| Operations       | Formulae                                           |
|------------------|----------------------------------------------------|
| Initialization   | $a_0 = a_{0\vert -1}$                              |
| Prediction       | $a_{t \vert t}\rightarrow a_{t+1 \vert t}=a_{t+1}$ |
| Update           | $a_{t \vert t-1}=a_t \rightarrow a_{t \vert t}$    |
| Prediction error | $e_t=y_t- \hat y_{t \vert t-1}$                    |
| Smoothing        | $a_{t \vert n} \rightarrow a_{t-1\vert n}$         |
| …                | ...                                                |

The relationships considered above are usually expressed under the form of matrices. All things considered, matrices are only a convenient way for describing linear transformations. By replacing matrix computations (at least the most frequent and/or the most expansive ones) with equivalent functions, it is possible to achieve substantial performance gain.

This is the basic principle of the JD+ state space framework: every type of model will have to provide a set of functions that allows an efficient computation of the different algorithms considered in the framework.

We shall clarify that point by an example. The prediction step is defined by the equations:

$$ a_{t+1 \vert t}=T_t a_{t \vert t}, \quad P_{t+1 \vert t}=T_t P_{t \vert t} T_t'+V_t=T_t \left(T_t P_{t \vert t} \right)'+V_t $$

The transformation $f_{T_t} (x)$ of an array (and by extension, of a matrix) by means of the matrix $T_t$ plays thus a central role in the forecasting step:

-   Apply $f_{T_t}$ on $a$
-   Apply $f_{T_t}$ on each column of $P$ ; we get $Q$
-   Apply $f_{T_t}$ on each row of $Q$ ; we get $O$
-   Add $V_t$ to $O$

In the case of (partially) time invariant systems, we will usually omit in the documentation of the framework the subscript $_t$ of the different arrays/matrices/functions.

Using matrix computation, the transformation $f_{T_t}$ needs $r \times r$ multiplications, where $r$ is the length of the state array. In many cases, the functional form will involve at most $r$ multiplications and much less memory traffic.

For instance, in some SSF forms of ARIMA models (see Gomez-Maravall, 1994), the function $y=f_T(x)$ is defined by

$$ y_i = \begin{cases} x_{i+1} & 0 \le i \lt r-1  \\ -\sum_{k=1}^p {\varphi_k x_{r-k} } & i =r-1\end{cases} $$

which corresponds to the transformation matrix

$$
\begin{pmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \ddots & 0 \\
0 & \cdots & \cdots & 0 & 1 \\
0 & \cdots & -\phi_p & - \cdots & -\phi_1
\end{pmatrix}
$$

It involves only $p$ (order of the autoregressive polynomial) multiplications.

As soon as direct matrix computations are avoided, the matrices of the system themselves don’t usually need to be created. That leads to another substantial gain in time and in memory space/traffic, especially in the case of time dependent systems.

## 2. Main algorithms

### 2.1 Stationary models

#### 2.1.1 Ordinary filter

#### 2.1.2 Ordinary smoother

### 2.2 Non-stationary models

## 3. Definition of the model by blocks

## 4. Description of the predefined blocks available in JD+

### 4.1 Atomic blocks

### 4.1.1 ARMA model

#### Introduction

The ARMA process is defined by

$$\Phi\left(B\right)y_t=\Theta\left(B\right)\epsilon_t $$

where:

$$\Phi\left(B\right)=1+\varphi_1 B + \cdots + \varphi_p B^p $$

$$\Theta\left(B\right)=1+\theta_1 B + \cdots + \theta_q B^q $$

are the auto-regressive and the moving average polynomials.

The MA representation of the model is $y_t=\sum_{i=0}^\infty {\psi_i \epsilon_{t-i}}$. Let $\gamma_i$ be the autocovariances of the model. We also define: $r=\max\left(p, q+1\right), \quad s=r-1$.

The state-space model can be written in different ways. JD+ provides two of them

#### Representation I

##### State vector

$$ \alpha_t= \begin{pmatrix} y_t \\ y_{t+1|t} \\ \vdots \\ y_{t+s|t} \end{pmatrix}$$

where $y_{t+i|t}$ is the orthogonal projection of $y_{t+i}$ on the subspace generated by ${y\left(s\right):s \leq t}$.Thus, it is the forecast function with respect to the semi-infinite sample. We also have that $y_{t+i|t} = \sum_{j=i}^\infty {\psi_j \epsilon_{t+i-j}}$

##### Dynamics

$$ T_t = \begin{pmatrix}0 &1 & 0 & \cdots & 0  \\0& 0 & 1 & \cdots & 0\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 0 & 0 & 0 & \cdots & 1\\
-\varphi_r & \cdots  & \cdots & \cdots &-\varphi_1 \end{pmatrix}$$

with $\varphi_{j}=0$ for $j>p$

$$ S_t = \begin{pmatrix}1 \\ \psi_1 \\ \vdots\\ \psi_s \end{pmatrix} $$

$$ V_t = S S' $$

##### Measurement

$$ Z_t = \begin{pmatrix} 1 & 0 & \cdots & 0\end{pmatrix}$$

$$ h_t = 0 $$

##### Initialization

$$ \alpha_{-1} = \begin{pmatrix}0 \\ 0 \\ \vdots\\ 0 \end{pmatrix} $$

$$ P_{*} = \Omega $$

$\Omega$ is the unconditional covariance of the state array; it can be easily derived using the MA representation. We have:

$$ \Omega\left(i,0\right) = \gamma_i $$

$$ \Omega\left(i,j\right) = \Omega\left(i-1,j-1\right)-\psi_i \psi_j $$

#### Representation II

We provide below an alternative state space representation as follows

##### State vector:

$$ \alpha_t= \begin{pmatrix} y_t \\ 
-\varphi_2 y_{t-1}-\dots-\varphi_r y_{t-r+1}+\theta_1 \epsilon_t+ \dots + \theta_{r-1} \epsilon_{t-r+2}\\ -\varphi_3 y_{t-1}-\dots-\varphi_r y_{t-r+2}+\theta_2 \epsilon_t+ \dots + \theta_{r-1} \epsilon_{t-r+3}\\ \vdots \\ 
-\varphi_r y_{t-1}+\theta_{r-1} \epsilon_{t}
\end{pmatrix}$$

##### Dynamics

$$ T_t = \begin{pmatrix}-\varphi_1 &1 & 0 & \cdots & 0  \\
-\varphi_2& 0 & 1 & \cdots & 0\\ 
\vdots & \vdots & \vdots & \ddots & \vdots\\ 
\vdots & 0 & 0 & \cdots & 1\\
-\varphi_r & 0  & \cdots & \cdots &0 \end{pmatrix}$$

$$ S_t = \begin{pmatrix}1 \\ \theta_1 \\ \vdots\\ \theta_r \end{pmatrix} $$

$$ V_t = S S' $$

##### Measurement

$$ Z_t = \begin{pmatrix} 1 & 0 & \cdots & 0\end{pmatrix}$$

$$ h_t = 0 $$

##### Initialization

$$ \alpha_{-1} = \begin{pmatrix}0 \\ 0 \\ \vdots\\ 0 \end{pmatrix} $$

$$ P_{*} = \Omega $$

$\Omega$ is the unconditional covariance of the state array; it can be easily derived using the MA representation. We have:

$$ \Omega\left(i,0\right) = \gamma_i $$

$$ \Omega\left(i,j\right) = \Omega\left(i-1,j-1\right)-\psi_i \psi_j $$

### 4.1.2 Time dependent ARMA model

#### Introduction

The time dependent ARMA process (TDARMA) is defined by

$$ \Phi_t\left(B\right)y_t=\Theta_t\left(B\right)\epsilon_t $$

where:

$$\Phi_t\left(B\right)=1+\varphi_{t,1} B + \cdots + \varphi_{t,p} B^p $$

$$\Theta_t\left(B\right)=1+\theta_{t,1} B + \cdots + \theta_{t,q} B^q $$

are the time dependent auto-regressive and the moving average polynomials.

#### State vector:

$$ \alpha_t= \begin{pmatrix} y_t \\ 
-\varphi_{t+1,2} y_{t-1}-\dots-\varphi_{t+1,r} y_{t-r+1}+\theta_{t+1,1} \epsilon_t+ \dots + \theta_{t+1, r-1} \epsilon_{t-r+2}\\ -\varphi_{t+2,3} y_{t-1}-\dots-\varphi_{t+2,r} y_{t-r+2}+\theta_{t+2,2 }\epsilon_t+ \dots + \theta_{t+2,r-1} \epsilon_{t-r+3}\\ \vdots \\ 
-\varphi_{t+r-1,r} y_{t-1}+\theta_{t+r-1,r-1} \epsilon_{t}
\end{pmatrix}$$

#### Dynamics

$$ \alpha_{t+1} = T_t \alpha_t + \begin{pmatrix} 1 \\ \theta_{t+2,1} \\ \theta_{t+3,2} \\ \vdots \\  \theta_{t+r, r-1} \\ \end{pmatrix} \epsilon_{t+1} $$

$$ T_t = \begin{pmatrix}-\varphi_{t+1,1} &1 & 0 & \cdots & 0  \\
-\varphi_{t+2,2} & 0 & 1 & \cdots & 0\\ 
\vdots & \vdots & \vdots & \ddots & \vdots\\ 
\vdots & 0 & 0 & \cdots & 1\\
-\varphi_{t+r,r} & 0  & \cdots & \cdots &0 \end{pmatrix}$$

$$ S_t = \begin{pmatrix}\sigma_{t+1}\\ \sigma_{t+1} \theta_{t+2,1} \\ \vdots\\ \sigma_{t+1} \theta_{t+r,r-1} \end{pmatrix} $$

$$ V_t = S S' $$

#### Measurement

$$ Z_t = \begin{pmatrix} 1 & 0 & \cdots & 0\end{pmatrix}$$

$$ h_t = 0 $$

#### Initialization

$$ \alpha_0= \begin{pmatrix} y_0 \\ 
-\varphi_{1,2} y_{-1}-\dots-\varphi_{1,r} y_{-r+1}+\theta_{1,1} \epsilon_0+ \dots + \theta_{1, r-1} \epsilon_{-r+2}\\ -\varphi_{2,3} y_{-1}-\dots-\varphi_{2,r} y_{-r+2}+\theta_{2,2 }\epsilon_0+ \dots + \theta_{2,r-1} \epsilon_{-r+3}\\ \vdots \\ 
-\varphi_{r-1,r} y_{-1}+\theta_{r-1,r-1} \epsilon_{0}
\end{pmatrix}$$

The covariance of $\alpha_0$ can be easily computed, taking into account that $y_0, \dots,y_{r-1}$ and $\epsilon_0, \dots, \epsilon_{r-1}$ are generated by the same processes and that their variances/covariances are:

$$ cov(y_i, y_j)=\gamma(i-j)$$

$$ cov(y_i, \epsilon_j) = \psi_{j-i}\sigma^2, j\le i$$

$$ cov(\epsilon_i, \epsilon_j)=\sigma^2, i=j $$

### 4.1.3 ARIMA model

#### Introduction

We consider that the auto-regressive polynomial contains some unit roots. It can be factorized in a stationary polynomial (defined by the roots outside the unit circle) and in a non-stationary polynomial (defined by the roots on the unit circle), which is notated:

$$ \Delta\left(B\right) = 1 + \delta_1 B + \cdots + \delta_d B^d $$

The state space form of an ARIMA model is similar to the state space form of an ARMA model, except for its initialization.

#### Initialization

The initial conditions can be written as follows:

$$ \alpha_{-1} = \begin{pmatrix}1 \\ 0 \\ \vdots\\ 0 \end{pmatrix} $$

$$ P_{*} = \Sigma \Omega \Sigma' $$

$$ B = \Lambda $$

$$ P_{\infty}= \Lambda \Lambda' $$

$\Omega$ is the unconditional covariance of the state array of the stationary model.

$$ \Sigma = \begin{pmatrix} 1 & 0 & \cdots & 0 \\ \lambda_1 & 1 & \cdots & 0 \\ \lambda_2 & \lambda_1 & \cdots & \vdots\\ \vdots & \vdots & \vdots & \vdots \\ \lambda_{r-1} & \lambda_{r-2} & \cdots & \lambda_{1} \end{pmatrix} $$

where $\lambda_{i}$ are generated by $\frac{1}{\Delta\left(B\right)}$

$\Lambda$ is a r x d matrix; its first d rows form an identity matrix; other cells are defined by the recursive relationship:

$$\Lambda \left(i,j\right) = -\sum_{k=1}^d {\delta_k \Lambda \left(i-k,j\right)}$$

### 4.1.3 Noise

### 4.1.4 Local level

### 4.1.5 Local linear trend

### 4.1.6 Seasonal component

### 4.1.7 Cyclical component

### 4.2 Complex blocks

## 5. Java design

## 6. R design
